{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgHLWu4Iq0sH"
   },
   "source": [
    "### Problem 1: Gradient Descent Update Rule\n",
    "\n",
    "**1.1**: In learning neural networks, we typically minimize a loss function $\\mathcal{L}(w)$ with respect to the network parameters $w$. It is also important that we *regularize* the network to reduce overfitting. A simple and popular regularization strategy is to penalize some *norm* of $w$.\n",
    "\n",
    "Consider that we have $N$ examples $(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)$ such that $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\{-1, 1\\}, i = 1...N$. Also consider that we have at our disposal a single neuron. Let $w = [w_1, w_2, ..., w_d]^T$ be the weight vector and the output be given by $\\hat y_i = tanh(w.x_i)$. The loss function is given by: $\\sum_{i=1}^N l(y_i, \\hat y_i) + \\lambda \\|w\\|^2$ where $\\lambda$ is the weight of regularization. Derive the update rule for minimizing this loss using stochastic gradient descent with step size $\\eta$ when $l(y_i, \\hat y_i) = log_e(1 + exp(-y_i. \\hat y_i))$. In other words, at time $t+1$, express the new parameters $w_{t+1}$ in terms of the old parameters $w_t$.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d8SHS4hq5UR"
   },
   "source": [
    "**1.1**:  \n",
    "\n",
    "The gradient of the given loss with respect to $w$ is:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_w \\big(l(y_i, \\hat y_i) + \\lambda \\|w\\|^2 \\big) &= \\nabla_w \\big(log(1 + exp(-y_i \\hat y_i))\\big) + 2\\lambda w \\\\\n",
    "&= \\nabla_{\\hat y_i} \\big(log(1 + exp(-y_i \\hat y_i))\\big) \\nabla_{w} (\\hat y_i) + 2\\lambda w \\\\\n",
    "&= \\frac{-y_i exp(-y_i \\hat y_i)}{1 + exp(-y_i \\hat y_i)} \\nabla_{w} (tanh(w.x_i)) + 2\\lambda w \\\\\n",
    "&= -y_i \\sigma(-y_i \\hat y_i) (1-tanh^2(w.x_i)) x_i + 2\\lambda w \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The corresponding update rule is:  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "w_{t+1} &= w_t - \\eta \\nabla_w \\big(l(y_i, \\hat y_i) + \\lambda \\|w\\|^2 \\big) \\\\\n",
    "&= w_t + \\eta y_i \\sigma(-y_i \\hat y_i) (1-tanh^2(w.x_i)) x_i - 2 \\eta \\lambda w \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrvD5wLZInKB"
   },
   "source": [
    "### Problem 2: Numerical Overflow and Underflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lpu0f3cAFqbv"
   },
   "source": [
    "The output obtained by running the below cell is due to the condition which is called [numerical underflow](https://en.wikipedia.org/wiki/Arithmetic_underflow). It is the condition that occurs when the true result of a floating point operation is smaller in magnitude than the smallest value representable as a normal floating point number in the target datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3haqHmNTFuBy",
    "outputId": "0cc6ebad-f01e-46be-dd61-cde0aee2d340"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e10+1e-10 == 1e10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuLC5pRtFyJu"
   },
   "source": [
    "The warning message given by running the below cell must have the word 'overflow' in it. This condition that occurs when a calculation produces a result that is greater in magnitude than the largest value representable in the target datatype is called [numerical overflow](https://en.wikipedia.org/wiki/Integer_overflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7MYoKBBFzjU",
    "outputId": "a6f3abb0-d17e-47dd-e450-a1d33bfd4cdf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-9a3a1acb84ee>:2: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(1000) == np.inf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.exp(1000) == np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1QXnciEF6dc"
   },
   "source": [
    "**2.1**: How do people deal with numerical overflow and underflow? Why have we implemented $\\text{softplus}(x) = \\log(1+\\exp(x))$ as shown in the cell below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tVfDG7dfF1e6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softplus(x_, limit=5):\n",
    "    x = np.array(x_)\n",
    "    compute_real_mask = np.logical_and(-limit < x, x < limit)\n",
    "    return_same_mask = x >= limit\n",
    "    computed_real_part = np.log(1 + np.exp(x*compute_real_mask)*compute_real_mask)\n",
    "    returned_same_part = x*return_same_mask\n",
    "    return computed_real_part + returned_same_part\n",
    "\n",
    "def test_softplus():\n",
    "    x_arr = np.linspace(-200, 200)\n",
    "    softplus_true = np.log(1 + np.exp(x_arr))\n",
    "    softplus_stable = softplus(x_arr)\n",
    "    assert np.max(abs(softplus_true-softplus_stable)) < 1e-3\n",
    "    \n",
    "test_softplus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJrAwp1CF1e8"
   },
   "source": [
    "**2.1**   \n",
    "\n",
    "One can deal with numerical underflow and overflow to some extent by using the datatype that can accomodate a wider range of numbers with high precision. However, one another way of dealing with numerical underflow and overflow is by changing the equation itself, which may lead to a loss in precision. The equation can be changed in such a way that the outcome is still valid and the loss in precision is within an acceptable level. \n",
    "\n",
    "$\\text{softplus}(x) = \\log(1+\\exp(x))$ is implemented as shown below in order to handle numerical underflow and overflow. $e^{x}$ is a very large number, when x is a large positive number and it is very small when x is a very small negative number. So, to calculate $\\log(1+e^x)$ in such cases, what can instead be done to handle underlow  and overflow is that the expression can be modified. When x is a very large positive number, $e^x+1 \\approx e^x$ and therefore, $\\log(1+e^x) \\approx x$. Similarly, when x is a very small negative number, $e^x+1 \\approx 1$ and therefore, $\\log(1+e^x) \\approx 0$. Of course, the resulting number has a loss of precision. Therefore, the code below uses these approximations only when x $\\geq$ 5 and x $\\leq$ -5 respectively. Taking this limit of 5 ensures that the drift from the actual value of the approximated value is within 1e-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcgYjUKKInKF"
   },
   "source": [
    "### Problem 3: Perceptron Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fjlfmcp1InKI"
   },
   "source": [
    "**3.1**: The code below generates $n$ 2D data points according to the Gaussian distribution $X1 \\sim \\mathcal{N}([1,0],\\,I_{2\\times2})$ and assigns them label 1. It also generates another $n$ 2D data points according to the Gaussian distribution $X2 \\sim \\mathcal{N}([-1,0],\\,I_{2\\times2})$ and assigns them label -1.\n",
    "\n",
    "If perceptron learning algorithm is used to classify the data, will it converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4MA0OGWYInKI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "def generate_data(n):\n",
    "    '''Generate synthetic data'''\n",
    "    \n",
    "    X1 = np.random.multivariate_normal([1,0], [[1,0],[0,1]], n)\n",
    "    X2 = np.random.multivariate_normal([-1,0], [[1,0],[0,1]], n)\n",
    "    X = np.vstack((X1, X2))\n",
    "\n",
    "    y1 = np.ones(n, dtype=int)\n",
    "    y2 = -np.ones(n, dtype=int)\n",
    "    y = np.concatenate((y1, y2))\n",
    "    \n",
    "    return X, y\n",
    "X_train, y_train = generate_data(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeHZwEQOd3TN"
   },
   "source": [
    "**3.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yHyV5jeXd3TO",
    "outputId": "941aa27d-b4e8-4acd-c0d3-c69997472fdd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "indexes_with_class1 = y_train==1\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Training data points')\n",
    "plt.scatter(X_train[indexes_with_class1][:,0],X_train[indexes_with_class1][:,1], color='r', marker='P')\n",
    "plt.scatter(X_train[~indexes_with_class1][:,0],X_train[~indexes_with_class1][:,1], color='g', marker='o')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7lGvZ7qd3TP"
   },
   "source": [
    "The data isn't linearly separable. So, perceptron learning algorithm will not converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icMuAi-Lr1CO"
   },
   "source": [
    "### Problem 4: Implementing Multilayer Perceptron and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZFH3r5liLSz"
   },
   "source": [
    "We will be using [MNIST dataset](http://yann.lecun.com/exdb/mnist/) of handwritten digits for this classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UKO-c29diLS0",
    "outputId": "6744aeaf-a045-4a89-99ce-fcf038aa181a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n",
      "Number of test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "## Load the training data from the data folder\n",
    "output_dim = 10\n",
    "x_train = np.load('./data/X_train.npy')\n",
    "x_train = x_train.flatten().reshape(-1,28*28)\n",
    "x_train = x_train / 255.0\n",
    "x_train = x_train.T\n",
    "gt_indices = np.load('./data/y_train.npy')\n",
    "train_length = x_train.shape[1]\n",
    "\n",
    "#creating one hot vector representation\n",
    "y_train = np.zeros((train_length, output_dim))\n",
    "for i in range(train_length):\n",
    "    y_train[i,gt_indices[i]] = 1\n",
    "y_train = y_train.T\n",
    "print(\"Number of training examples: {:d}\".format(train_length))\n",
    "\n",
    "## Loading the test data\n",
    "x_test = np.load('./data/X_test.npy')\n",
    "x_test = x_test.flatten().reshape(-1,28*28)\n",
    "x_test = x_test / 255.0\n",
    "x_test = x_test.T\n",
    "gt_indices = np.load('./data/y_test.npy')\n",
    "test_length = x_test.shape[1]\n",
    "\n",
    "#creating one hot vector representation\n",
    "y_test = np.zeros((test_length, output_dim))\n",
    "for i in range(test_length):\n",
    "    y_test[i,gt_indices[i]] = 1\n",
    "y_test = y_test.T\n",
    "print(\"Number of test examples: {:d}\".format(test_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cgce6j2MiLS1"
   },
   "source": [
    "This problem considers neural networks with multiple layers. Each layer has multiple inputs and outputs, and can be broken down into two parts:  \n",
    "\n",
    "A linear module that implements a linear transformation:     $ z_j = (\\sum^{m}_{i=1} x_i W_{i,j}) + {W_0}_j$  \n",
    "specified by a weight matrix $W$ and a bias vector $W_0$. The output is $[z_1, \\ldots, z_n]^T$\n",
    "\n",
    "An activation module that applies an activation function to the outputs of the linear module for some activation function $f$, such as Tanh or ReLU in the hidden layers or Softmax at the output layer. We write the output as: $[f(z_1), \\ldots, f(z_m)]^T$, although technically, for some activation functions such as softmax, each output will depend on all the $z_i$.\n",
    "\n",
    "We will use the following notation for quantities in a network:\n",
    "- Inputs to the network are $x_1,..., x_d$\n",
    "- Number of layers is $L$\n",
    "- There are $m^l$ inputs to layer $l$\n",
    "- There are $n^l = m^{l+1}$ outputs from layer $l$\n",
    "- The weight matrix for layer $l$ is $W^l$, an $m^l \\times n^l$ matrix, and the bias vector (offset) is $W_0^l$, an $n^l \\times 1$ vector\n",
    "- The outputs of the linear module for layer $l$ are known as pre-activation values and denoted $z^l$\n",
    "- The activation function at layer $l$ is $f^l(\\cdot)$\n",
    "- Layer $l$ activations are $a^l = [f^l(z^l_1), \\ldots, f^l(z^l_{n^l})]^T$\n",
    "- The output of the network is the values $a^L = [f^L(z^L_1), \\ldots, f^L(z^L_{n^L})]^T$\n",
    "- Loss function $Loss(a,y)$ measures the loss of output values $a$ when the target is $y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRIPVRhviLS1"
   },
   "source": [
    "We'll use the modular implementation, which leads to clean code. The basic framework for SGD training is given below. We can construct a network and train it as follows:\n",
    "\n",
    "```\n",
    "# build a 3-layer network\n",
    "net = Sequential([Linear(2,3), Tanh(),\n",
    "                  Linear(3,3), Tanh(),\n",
    "    \t          Linear(3,2), SoftMax()])\n",
    "# train the network on data and labels\n",
    "net.sgd(x_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cY3OEZWRiLS1"
   },
   "source": [
    "## Linear Modules: ##\n",
    "Each linear module has a forward method that takes in a batch of activations A (from the previous layer) and returns a batch of pre-activations Z; ; it can also store its input or output vectors for use by other methods (e.g., for subsequent backpropagation).\n",
    "\n",
    "$Z = W^T A + W_0$\n",
    "\n",
    "Each linear module has a backward method that takes in a column vector dLdZ and returns dLdA. This module also computes and stores dLdW and dLdW0, the gradients with respect to the weights.\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial A} = \\frac{\\partial Z}{\\partial A} \\frac{\\partial Loss}{\\partial Z}$ and similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HENTHEIPiLS1"
   },
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def sgd_step(self, lrate): \n",
    "        pass # For modules without weights\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, m, n):\n",
    "        self.m, self.n = (m, n)  # (in size, out size)\n",
    "        self.W0 = np.zeros([self.n, 1])  # (n x 1)\n",
    "        self.W = np.random.normal(0, 1.0 * m ** (-.5), [m, n])  # (m x n)\n",
    "\n",
    "    def forward(self, A):\n",
    "        self.A = A   # (m x b)  b is the batch size, which is 1 for SGD\n",
    "        return np.dot(self.W.T, A) + self.W0\n",
    "\n",
    "    def backward(self, dLdZ):  # dLdZ is (n x b), uses stored self.A\n",
    "        self.dLdW = np.dot(self.A, dLdZ.T) # (m x n)\n",
    "        self.dLdW0 = dLdZ.sum(axis=1).reshape((self.n, 1)) # (n x 1)\n",
    "        return np.dot(self.W, dLdZ) # (m x b)\n",
    "\n",
    "    def sgd_step(self, lrate):  # Gradient descent step\n",
    "        self.W -= lrate*self.dLdW\n",
    "        self.W0 -= lrate*self.dLdW0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_na1Jr-siLS2"
   },
   "source": [
    "## Activation functions: ##\n",
    "Activation modules don't have any weights and so they are simpler.\n",
    "\n",
    "Each activation module has a forward method that takes in a batch of pre-activations Z and returns a batch of activations A.\n",
    "\n",
    "Each activation module has a backward method that takes in dLdA and returns dLdZ, with the exception of SoftMax, where we assume dLdZ is passed in.\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial Z} = \\frac{\\partial Loss}{\\partial A} \\frac{\\partial A}{\\partial Z}$\n",
    "\n",
    "For Softmax = $SM(Z)$ at the output layer and cross entropy as the $Loss(A,Y)$ function, there is a [simple form](https://peterroelants.github.io/posts/cross-entropy-softmax/) for ${\\tt dLdZ} = \\frac{\\partial Loss}{\\partial Z}$; namely, it is the prediction error $A−Y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bY3HFGcHCkj"
   },
   "source": [
    "### Tanh: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dFD9yB38iLS3"
   },
   "outputs": [],
   "source": [
    "class Tanh(Module):            # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = np.tanh(Z)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):    # Uses stored self.A\n",
    "        return dLdA * (1.0 - (self.A ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkBAcTbyiLS3"
   },
   "source": [
    "### ReLU: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WNZgbM1ViLS3"
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):              # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = np.maximum(0, Z)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):    # uses stored self.A\n",
    "        return dLdA * (self.A != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qVlW8SFiLS3"
   },
   "source": [
    "### SoftMax: ###\n",
    "For `SoftMax.class_fun()`, given the column vector of class probabilities for each point (computed by Softmax), return a vector of the classes (integers) with the highest probability for each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6KrMWcgwiLS4"
   },
   "outputs": [],
   "source": [
    "class SoftMax(Module):           # Output activation\n",
    "    def forward(self, Z):\n",
    "        return np.exp(Z) / np.sum(np.exp(Z), axis=0)\n",
    "\n",
    "    def backward(self, dLdZ):    # Assume that dLdZ is passed in\n",
    "        return dLdZ\n",
    "\n",
    "    def class_fun(self, Ypred):  # Return class indices\n",
    "        return np.argmax(Ypred, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzwrK1XsiLS4"
   },
   "source": [
    "## Loss Function: ##\n",
    "Each loss module has a forward method that takes in a batch of predictions Ypred (from the previous layer) and labels Y and returns a scalar loss value.\n",
    "\n",
    "The CrossE module has a backward method that returns dLdZ, the gradient with respect to the preactivation to SoftMax (note: not the activation!), since we are always pairing SoftMax activation with Cross Entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTp5-_H6iLS5"
   },
   "source": [
    "### Cross Entropy: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "E3t9-M8diLS5"
   },
   "outputs": [],
   "source": [
    "class CROSSE(Module):       # Loss\n",
    "    def forward(self, Ypred, Y):\n",
    "        self.Ypred = Ypred\n",
    "        self.Y = Y\n",
    "        return float(np.sum(-Y * np.log(Ypred)))\n",
    "\n",
    "    def backward(self):  # Use stored self.Ypred, self.Y\n",
    "        return self.Ypred - self.Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XrtkyTNiLS5"
   },
   "source": [
    "## Neural Network: ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zFNsXNmiLS6"
   },
   "source": [
    "Implement SGD. \n",
    "- We randomly pick a data point Xt, Yt by using np.random.randint to choose a random index into the data. \n",
    "- Then we compute the predicted output Ypred for Xt with the forward method and the loss for Ypred relative to Yt. \n",
    "- We use the backward method to compute the gradients and the sgd_step method to change the weights.\n",
    "\n",
    "Also, we record the training accuracy after every 1000 iterations and plot it to show how the training accuracy changes with the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JSPrY1tViLS6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, modules, loss):            # List of modules, loss module\n",
    "        self.modules = modules\n",
    "        self.loss = loss\n",
    "\n",
    "    def sgd(self, X, Y, iters=100, lrate=0.005):  # Train\n",
    "        D, N = X.shape\n",
    "        accuracies = []\n",
    "        itrns = []\n",
    "        for it in range(iters):\n",
    "            i = np.random.randint(N)\n",
    "            Xt = X[:, i:i+1]\n",
    "            Yt = Y[:, i:i+1]\n",
    "            Ypred = self.forward(Xt)\n",
    "            loss = self.loss.forward(Ypred, Yt)\n",
    "            err = self.loss.backward()\n",
    "            self.backward(err)\n",
    "            self.sgd_step(lrate)\n",
    "            if it % 1000 == 0 or it == iters-1:\n",
    "                acc = self.get_accuracy(X, Y)\n",
    "                accuracies.append(acc * 100)\n",
    "                itrns.append(it)\n",
    "                print('Iteration =', it, '\\tTraining Accuracy = %.2f%%' % (acc * 100))\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Training Accuracy')\n",
    "        plt.plot(itrns, accuracies)\n",
    "\n",
    "    def forward(self, Xt):                        # Compute Ypred\n",
    "        for m in self.modules: \n",
    "            Xt = m.forward(Xt)\n",
    "        return Xt\n",
    "\n",
    "    def backward(self, delta):                    # Update dLdW and dLdW0\n",
    "        # Note reversed list of modules\n",
    "        for m in self.modules[::-1]: \n",
    "            delta = m.backward(delta)\n",
    "\n",
    "    def sgd_step(self, lrate):                    # Gradient descent step\n",
    "        for m in self.modules: \n",
    "            m.sgd_step(lrate)\n",
    "\n",
    "    def get_accuracy(self, X, Y):\n",
    "        # Method to print accuracy\n",
    "        cf = self.modules[-1].class_fun\n",
    "        acc = np.mean(cf(self.forward(X)) == cf(Y))\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uptp9jRLd3TV"
   },
   "source": [
    "Now, keeping the **number of hidden layers fixed at 2** and **learning rate fixed at 0.005** and the **number of iterations fixed at 17000**, we try tuning the number of hidden units in each hidden layer as well as the activation function after every linear module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6fpO_iwyiLS6",
    "outputId": "25bb3c58-1cb9-447d-923f-db47219a4d27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 0 \tTraining Accuracy = 11.46%\n",
      "Iteration = 1000 \tTraining Accuracy = 78.23%\n",
      "Iteration = 2000 \tTraining Accuracy = 85.13%\n",
      "Iteration = 3000 \tTraining Accuracy = 87.58%\n",
      "Iteration = 4000 \tTraining Accuracy = 85.98%\n",
      "Iteration = 5000 \tTraining Accuracy = 88.33%\n",
      "Iteration = 6000 \tTraining Accuracy = 88.25%\n",
      "Iteration = 7000 \tTraining Accuracy = 89.91%\n",
      "Iteration = 8000 \tTraining Accuracy = 89.85%\n",
      "Iteration = 9000 \tTraining Accuracy = 90.74%\n",
      "Iteration = 10000 \tTraining Accuracy = 90.79%\n",
      "Iteration = 11000 \tTraining Accuracy = 92.13%\n",
      "Iteration = 12000 \tTraining Accuracy = 90.64%\n",
      "Iteration = 13000 \tTraining Accuracy = 91.44%\n",
      "Iteration = 14000 \tTraining Accuracy = 92.38%\n",
      "Iteration = 15000 \tTraining Accuracy = 92.66%\n",
      "Iteration = 16000 \tTraining Accuracy = 92.56%\n",
      "Iteration = 16999 \tTraining Accuracy = 93.47%\n",
      "Test Acc = 93.34%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_dim = 784  # input dimension\n",
    "nn = Sequential([Linear(input_dim, 256), ReLU(), Linear(256, 128), ReLU(), Linear(128,output_dim), SoftMax()], CROSSE())\n",
    "nn.sgd(x_train, y_train, iters=17000, lrate=0.005)\n",
    "test_acc = nn.get_accuracy(x_test, y_test)\n",
    "print('Test Acc = %.2f%%' % (test_acc * 100))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Solutions_Problem_Set_02.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
